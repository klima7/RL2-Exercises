{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Laboratorium 7\n","\n","Celem siódmego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmu głębokiego uczenia aktywnego - Actor-Critic. Zaimplementowany algorytm będzie testowany z wykorzystaniem środowiska z OpenAI - *CartPole*.\n"]},{"cell_type":"markdown","metadata":{},"source":["Dołączenie standardowych bibliotek"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["from collections import deque\n","import gym\n","import numpy as np\n","import random"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow.keras as keras\n","import tensorflow.keras.layers as layers"]},{"cell_type":"markdown","metadata":{},"source":["Dołączenie bibliotek do obsługi sieci neuronowych"]},{"cell_type":"markdown","metadata":{},"source":["## Zadanie 1 - Actor-Critic\n","\n","<p style='text-align: justify;'>\n","Celem ćwiczenie jest zaimplementowanie algorytmu Actor-Critic. W tym celu należy utworzyć dwie głębokie sieci neuronowe:\n","    1. *actor* - sieć, która będzie uczyła się optymalnej strategii (podobna do tej z laboratorium 6),\n","    2. *critic* - sieć, która będzie uczyła się funkcji oceny stanu (podobnie jak się DQN).\n","Wagi sieci *actor* aktualizowane są zgodnie ze wzorem:\n","\\begin{equation*}\n","    \\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_\\theta log \\pi_{\\theta}(a_t, s_t | \\theta).\n","\\end{equation*}\n","Wagi sieci *critic* aktualizowane są zgodnie ze wzorem:\n","\\begin{equation*}\n","    w \\leftarrow w + \\beta \\delta_t \\nabla_w\\upsilon(s_{t + 1}, w),\n","\\end{equation*}\n","gdzie:\n","\\begin{equation*}\n","    \\delta_t \\leftarrow r_t + \\gamma \\upsilon(s_{t + 1}, w) - \\upsilon(s_t, w).\n","\\end{equation*}\n","</p>"]},{"cell_type":"code","execution_count":24,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam\n","\n","class REINFORCEAgent:\n","    def __init__(self, state_size, action_size, actor, critic):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.gamma = 0.99    # discount rate\n","        self.actor = actor\n","        self.critic = critic #critic network should have only one output\n","        self.actor_optimizer = Adam(learning_rate=0.0001)\n","        self.critic_optimizer = Adam(learning_rate=0.0005)\n","\n","\n","    def get_action(self, state):\n","        \"\"\"\n","        Compute the action to take in the current state, basing on policy returned by the network.\n","\n","        Note: To pick action according to the probability generated by the network\n","        \"\"\"\n","\n","        #\n","        # INSERT CODE HERE to get action in a given state\n","        #        \n","        predictions = self.actor.predict_on_batch(np.array([state]))[0]\n","        return random.choices(range(len(predictions)), weights=predictions)[0]\n","\n","\n","    def learn(self, state, action, reward, next_state, done):\n","        \"\"\"\n","        Function learn networks using information about state, action, reward and next state. \n","        First the values for \n","        Critic network should be trained based on target value:state and next_state should be estimated based on output of critic network.\n","        target = r + \\gamma next_state_value if not done]\n","        target = r if done.\n","        Actor network shpuld be trained based on delta value:\n","        delta = target - state_value\n","        \"\"\"\n","        #\n","        # INSERT CODE HERE to train network\n","        #\n","        critic_response = self.critic.predict_on_batch(np.array([state, next_state]))\n","        delta = reward + self.gamma * critic_response[1][0] - critic_response[0][0] if not done else reward\n","        self.__update_actor(delta, state, action)\n","        self.__update_critic(delta, next_state)\n","        \n","    def __update_actor(self, delta, state, action):\n","        with tf.GradientTape() as tape:\n","            states = np.array([state])\n","            actions = np.array([action])\n","            \n","            policy = self.actor(np.array(states))\n","            actions_one_hot = tf.keras.utils.to_categorical(actions, num_classes=self.action_size)\n","            log_probabilities = tf.math.log(tf.reduce_sum(tf.multiply(policy, actions_one_hot), axis=1))\n","            loss = -tf.reduce_mean(log_probabilities * delta)\n","            \n","        gradients = tape.gradient(loss, self.actor.trainable_variables)\n","        self.actor_optimizer.apply_gradients(zip(gradients, self.actor.trainable_variables))\n","        \n","    def __update_critic(self, delta, next_state):\n","        with tf.GradientTape() as tape:\n","            predictions = self.critic(np.array([next_state]))[0]\n","            loss = tf.multiply(delta, predictions)\n","            \n","        gradients = tape.gradient(loss, self.critic.trainable_variables)\n","        self.critic_optimizer.apply_gradients(zip(gradients, self.critic.trainable_variables))"]},{"cell_type":"markdown","metadata":{},"source":["Czas przygotować model sieci, która będzie się uczyła działania w środowisku [*CartPool*](https://gym.openai.com/envs/CartPole-v0/):"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["env = gym.make(\"CartPole-v0\").env\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["actor_model = keras.models.Sequential([\n","    layers.InputLayer((state_size,)),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(action_size, activation='softmax')\n","])\n","\n","# actor_model.compile(optimizer='adam', loss='mse')"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["critic_model = keras.models.Sequential([\n","    layers.InputLayer((state_size,)),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(32, activation='relu'),\n","    layers.Dense(1)\n","])\n","\n","# critic_model.compile(optimizer='adam', loss='mse')"]},{"cell_type":"markdown","metadata":{},"source":["Czas nauczyć agenta gry w środowisku *CartPool*:"]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021B794728B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","mean reward:64.630\n","mean reward:77.010\n","mean reward:67.730\n","mean reward:53.040\n","mean reward:24.460\n","mean reward:14.930\n","mean reward:16.480\n","mean reward:11.940\n","mean reward:12.290\n","mean reward:12.350\n","mean reward:13.070\n","mean reward:13.330\n","mean reward:12.680\n","mean reward:13.200\n","mean reward:11.010\n","mean reward:11.600\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Lukasz\\OneDrive - Politechnika Łódzka\\siium\\semestr-3\\guzw\\notebooks\\Lab. 7.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukasz/OneDrive%20-%20Politechnika%20%C5%81%C3%B3dzka/siium/semestr-3/guzw/notebooks/Lab.%207.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukasz/OneDrive%20-%20Politechnika%20%C5%81%C3%B3dzka/siium/semestr-3/guzw/notebooks/Lab.%207.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Lukasz/OneDrive%20-%20Politechnika%20%C5%81%C3%B3dzka/siium/semestr-3/guzw/notebooks/Lab.%207.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mget_action(state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukasz/OneDrive%20-%20Politechnika%20%C5%81%C3%B3dzka/siium/semestr-3/guzw/notebooks/Lab.%207.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukasz/OneDrive%20-%20Politechnika%20%C5%81%C3%B3dzka/siium/semestr-3/guzw/notebooks/Lab.%207.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     agent\u001b[39m.\u001b[39mlearn(state, action, reward, next_state, done)\n","\u001b[1;32mc:\\Users\\Lukasz\\OneDrive - Politechnika Łódzka\\siium\\semestr-3\\guzw\\notebooks\\Lab. 7.ipynb Cell 13\u001b[0m in \u001b[0;36mREINFORCEAgent.get_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukasz/OneDrive%20-%20Politechnika%20%C5%81%C3%B3dzka/siium/semestr-3/guzw/notebooks/Lab.%207.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukasz/OneDrive%20-%20Politechnika%20%C5%81%C3%B3dzka/siium/semestr-3/guzw/notebooks/Lab.%207.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mCompute the action to take in the current state, basing on policy returned by the network.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukasz/OneDrive%20-%20Politechnika%20%C5%81%C3%B3dzka/siium/semestr-3/guzw/notebooks/Lab.%207.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukasz/OneDrive%20-%20Politechnika%20%C5%81%C3%B3dzka/siium/semestr-3/guzw/notebooks/Lab.%207.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mNote: To pick action according to the probability generated by the network\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukasz/OneDrive%20-%20Politechnika%20%C5%81%C3%B3dzka/siium/semestr-3/guzw/notebooks/Lab.%207.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukasz/OneDrive%20-%20Politechnika%20%C5%81%C3%B3dzka/siium/semestr-3/guzw/notebooks/Lab.%207.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukasz/OneDrive%20-%20Politechnika%20%C5%81%C3%B3dzka/siium/semestr-3/guzw/notebooks/Lab.%207.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# INSERT CODE HERE to get action in a given state\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukasz/OneDrive%20-%20Politechnika%20%C5%81%C3%B3dzka/siium/semestr-3/guzw/notebooks/Lab.%207.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#        \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Lukasz/OneDrive%20-%20Politechnika%20%C5%81%C3%B3dzka/siium/semestr-3/guzw/notebooks/Lab.%207.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor\u001b[39m.\u001b[39;49mpredict_on_batch(np\u001b[39m.\u001b[39;49marray([state]))[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Lukasz/OneDrive%20-%20Politechnika%20%C5%81%C3%B3dzka/siium/semestr-3/guzw/notebooks/Lab.%207.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m random\u001b[39m.\u001b[39mchoices(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(predictions)), weights\u001b[39m=\u001b[39mpredictions)[\u001b[39m0\u001b[39m]\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["agent = REINFORCEAgent(state_size, action_size, actor_model, critic_model)\n","\n","\n","for i in range(100):\n","    score_history = []\n","\n","    for i in range(100):\n","        done = False\n","        score = 0\n","        state = env.reset()\n","        while not done:\n","            action = agent.get_action(state)\n","            next_state, reward, done, _ = env.step(action)\n","            agent.learn(state, action, reward, next_state, done)\n","            state = next_state\n","            score += reward\n","        score_history.append(score)\n","\n","    print(\"mean reward:%.3f\" % (np.mean(score_history)))\n","\n","    if np.mean(score_history) > 300:\n","        print(\"You Win!\")\n","        break"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
