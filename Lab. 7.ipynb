{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Laboratorium 7\n","\n","Celem siódmego laboratorium jest zapoznanie się oraz zaimplementowanie algorytmu głębokiego uczenia aktywnego - Actor-Critic. Zaimplementowany algorytm będzie testowany z wykorzystaniem środowiska z OpenAI - *CartPole*.\n"]},{"cell_type":"markdown","metadata":{},"source":["Dołączenie standardowych bibliotek"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["from collections import deque\n","import gym\n","import numpy as np\n","import random"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow.keras as keras\n","import tensorflow.keras.layers as layers"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  0\n"]}],"source":["print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"]},{"cell_type":"markdown","metadata":{},"source":["Dołączenie bibliotek do obsługi sieci neuronowych"]},{"cell_type":"markdown","metadata":{},"source":["## Zadanie 1 - Actor-Critic\n","\n","<p style='text-align: justify;'>\n","Celem ćwiczenie jest zaimplementowanie algorytmu Actor-Critic. W tym celu należy utworzyć dwie głębokie sieci neuronowe:\n","    1. *actor* - sieć, która będzie uczyła się optymalnej strategii (podobna do tej z laboratorium 6),\n","    2. *critic* - sieć, która będzie uczyła się funkcji oceny stanu (podobnie jak się DQN).\n","Wagi sieci *actor* aktualizowane są zgodnie ze wzorem:\n","\\begin{equation*}\n","    \\theta \\leftarrow \\theta + \\alpha \\delta_t \\nabla_\\theta log \\pi_{\\theta}(a_t, s_t | \\theta).\n","\\end{equation*}\n","Wagi sieci *critic* aktualizowane są zgodnie ze wzorem:\n","\\begin{equation*}\n","    w \\leftarrow w + \\beta \\delta_t \\nabla_w\\upsilon(s_{t + 1}, w),\n","\\end{equation*}\n","gdzie:\n","\\begin{equation*}\n","    \\delta_t \\leftarrow r_t + \\gamma \\upsilon(s_{t + 1}, w) - \\upsilon(s_t, w).\n","\\end{equation*}\n","</p>"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam\n","\n","class REINFORCEAgent:\n","    def __init__(self, state_size, action_size, actor, critic):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.gamma = 0.99    # discount rate\n","        self.actor = actor\n","        self.critic = critic #critic network should have only one output\n","        self.actor_optimizer = Adam(learning_rate=0.0001)\n","        self.critic_optimizer = Adam(learning_rate=0.0005)\n","\n","\n","    def get_action(self, state):\n","        \"\"\"\n","        Compute the action to take in the current state, basing on policy returned by the network.\n","\n","        Note: To pick action according to the probability generated by the network\n","        \"\"\"\n","\n","        #\n","        # INSERT CODE HERE to get action in a given state\n","        #        \n","        predictions = self.actor.predict_on_batch(np.array([state]))[0]\n","        return random.choices(range(len(predictions)), weights=predictions)[0]\n","\n","\n","    def learn(self, state, action, reward, next_state, done):\n","        \"\"\"\n","        Function learn networks using information about state, action, reward and next state. \n","        First the values for \n","        Critic network should be trained based on target value:state and next_state should be estimated based on output of critic network.\n","        target = r + \\gamma next_state_value if not done]\n","        target = r if done.\n","        Actor network shpuld be trained based on delta value:\n","        delta = target - state_value\n","        \"\"\"\n","        #\n","        # INSERT CODE HERE to train network\n","        #\n","        critic_response = self.critic.predict_on_batch(np.array([state, next_state]))\n","        delta = reward + self.gamma * (critic_response[1][0] if not done else 0) - critic_response[0][0]\n","        self.__update_critic(delta, state)\n","        self.__update_actor(delta, state, action)\n","        \n","    def __update_critic(self, delta, state):\n","        with tf.GradientTape() as tape:\n","            predictions = self.critic(np.array([state]))[0]\n","            loss = - tf.multiply(delta, predictions)\n","            \n","        gradients = tape.gradient(loss, self.critic.trainable_variables)\n","        self.critic_optimizer.apply_gradients(zip(gradients, self.critic.trainable_variables))\n","        \n","    def __update_actor(self, delta, state, action):\n","        with tf.GradientTape() as tape:            \n","            policy = self.actor(np.array([state]))[0]\n","            loss = - delta * tf.math.log(policy[action])\n","            \n","        gradients = tape.gradient(loss, self.actor.trainable_variables)\n","        self.actor_optimizer.apply_gradients(zip(gradients, self.actor.trainable_variables))"]},{"cell_type":"markdown","metadata":{},"source":["Czas przygotować model sieci, która będzie się uczyła działania w środowisku [*CartPool*](https://gym.openai.com/envs/CartPole-v0/):"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["env = gym.make(\"CartPole-v0\").env\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["actor_model = keras.models.Sequential([\n","    layers.InputLayer((state_size,)),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(action_size, activation='softmax')\n","])"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["critic_model = keras.models.Sequential([\n","    layers.InputLayer((state_size,)),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(32, activation='relu'),\n","    layers.Dense(1)\n","])"]},{"cell_type":"markdown","metadata":{},"source":["Czas nauczyć agenta gry w środowisku *CartPool*:"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mean reward:18.420\n","mean reward:35.390\n","mean reward:116.940\n","mean reward:363.910\n","You Win!\n"]}],"source":["agent = REINFORCEAgent(state_size, action_size, actor_model, critic_model)\n","from tqdm import tqdm\n","\n","\n","for i in range(100):\n","    score_history = []\n","\n","    for i in range(100):\n","        done = False\n","        score = 0\n","        state = env.reset()\n","        while not done:\n","            action = agent.get_action(state)\n","            next_state, reward, done, _ = env.step(action)\n","            agent.learn(state, action, reward, next_state, done)\n","            state = next_state\n","            score += reward\n","        score_history.append(score)\n","\n","    print(\"mean reward:%.3f\" % (np.mean(score_history)))\n","\n","    if np.mean(score_history) > 300:\n","        print(\"You Win!\")\n","        break"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
